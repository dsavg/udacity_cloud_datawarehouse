{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import psycopg2\n",
    "from sql_queries import copy_table_queries, insert_table_queries\n",
    "from sql_queries import create_table_queries, drop_table_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('creds.cfg'))\n",
    "\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "HOST                   = config.get(\"CLUSTER\",\"HOST\")\n",
    "DB_NAME                = config.get(\"CLUSTER\",\"DB_NAME\")\n",
    "DB_USER                = config.get(\"CLUSTER\",\"DB_USER\")\n",
    "DB_PASSWORD            = config.get(\"CLUSTER\",\"DB_PASSWORD\")\n",
    "DB_PORT                = config.get(\"CLUSTER\",\"DB_PORT\")\n",
    "\n",
    "IAM_ROLE_NAME          = config.get(\"IAM_ROLE\", \"IAM_ROLE_NAME\")\n",
    "\n",
    "LOG_DATA               = config.get(\"S3\", \"LOG_DATA\")\n",
    "LOG_JSONPATH           = config.get(\"S3\", \"LOG_JSONPATH\")\n",
    "SONG_DATA              = config.get(\"S3\", \"SONG_DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create redshift client\n",
    "redshift = boto3.client('redshift',\n",
    "                        region_name='us-west-2', \n",
    "                        aws_access_key_id=KEY, \n",
    "                        aws_secret_access_key=SECRET\n",
    "                       )\n",
    "# create ec2 client\n",
    "ec2 = boto3.resource('ec2',\n",
    "                    region_name='us-west-2', \n",
    "                    aws_access_key_id=KEY, \n",
    "                    aws_secret_access_key=SECRET\n",
    "                   )\n",
    "# create s3 client\n",
    "s3 = boto3.resource('s3',\n",
    "                    region_name='us-west-2', \n",
    "                    aws_access_key_id=KEY, \n",
    "                    aws_secret_access_key=SECRET\n",
    "                   )\n",
    "\n",
    "# create iam client\n",
    "iam = boto3.client('iam',\n",
    "                    region_name='us-west-2', \n",
    "                    aws_access_key_id=KEY, \n",
    "                    aws_secret_access_key=SECRET\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check sample s3 data\n",
    "sampleDbBucket = s3.Bucket(\"udacity-dend\")\n",
    "\n",
    "for object_summary in sampleDbBucket.objects.all():\n",
    "    print(object_summary.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the IAM role\n",
    "try:\n",
    "    print('1.1 Creating a new IAM Role')\n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/', \n",
    "        RoleName=IAM_ROLE_NAME, \n",
    "        Description='Allows Redshift clusters to call AWS services on your behalf.', \n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "             'Version': '2012-10-17'})\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Attach Policy\n",
    "print('1.2 Attaching Policy')\n",
    "iam.attach_role_policy(RoleName=IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']\n",
    "\n",
    "# Get and print the IAM role ARN\n",
    "print('1.3 Get the IAM role ARN')\n",
    "roleArn = iam.get_role(RoleName=IAM_ROLE_NAME)['Role']['Arn']\n",
    "\n",
    "print(roleArn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a redshift cluster\n",
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "        # add parameters for hardware\n",
    "        ClusterType='multi-node', \n",
    "        NodeType='dc2.large', \n",
    "        NumberOfNodes=int(4), \n",
    "        # add parameters for identifiers & credentials\n",
    "        DBName=DB_NAME, \n",
    "        ClusterIdentifier=HOST, \n",
    "        MasterUsername==DB_USER, \n",
    "        MasterUserPassword=DB_PASSWORD, \n",
    "        # add parameter for role (to allow s3 access)\n",
    "        IamRoles=[roleArn]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# check if the cluster is available\n",
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \n",
    "                             \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster endpoint and role ARN\n",
    "DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "print(\"DWH_ENDPOINT :: \", DWH_ENDPOINT)\n",
    "print(\"DWH_ROLE_ARN :: \", DWH_ROLE_ARN)\n",
    "\n",
    "# open incoming TCP port to access the cluster endpoint\n",
    "try:\n",
    "    vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    \n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName=defaultSg.group_name,\n",
    "        CidrIp='0.0.0.0/0',  \n",
    "        IpProtocol='TCP',  \n",
    "        FromPort=int(DWH_PORT),\n",
    "        ToPort=int(DWH_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# connect to cluster\n",
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_string=f\"postgresql://{DWH_DB_USER}:{DWH_DB_PASSWORD}@{DWH_ENDPOINT}:{DWH_PORT}/{DWH_DB}\"\n",
    "print(conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql $conn_string"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
